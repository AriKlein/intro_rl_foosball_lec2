{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Rediscovering_RL_Notebook_0_SOLVED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriKlein/intro_rl_foosball_lec2/blob/main/W3S2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv9hgPks9LhM"
      },
      "source": [
        "# Reinforcement learning with Foolsball\n",
        "- Reinforcement learning is learning to make decisions from experience.\n",
        "- Games are a good testbed for RL.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWEnCb0m9ve8"
      },
      "source": [
        "# About Foolsball\n",
        "- 5x4 playground that provides a football/foosball-like environment.\n",
        "- A controllable player:\n",
        "  - always spawned in the top-left corner\n",
        "  - displayed as '⚽'\n",
        "  - can move North, South, East or West.\n",
        "  - can be controlled algorithmically\n",
        "- A number of **static** opponents, each represented by 👕, that occupy certain locations on the field.\n",
        "- A goalpost 🥅 that is fixed in the bottom right corner\n",
        "\n",
        "## Goals\n",
        "### Primary goal\n",
        "- We want the agent to learn to reach the goalpost \n",
        "\n",
        "### Secondary goals\n",
        "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. \n",
        "\n",
        "## Rules \n",
        "- Initial rules:\n",
        "    - The ball can be (tried to be) moved in four direction: \\['n','e','w',s'\\]\n",
        "    - Move the ball to an unmarked position: -1 points\n",
        "    - Move the ball to a position marked by a defender: -5 points\n",
        "    - Try to move the ball ouside the field: -1 (ball stays in the previous position)\n",
        "    - Move the ball into the goal post position: +5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GseOw35JtwO4"
      },
      "source": [
        "# Create the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InnWTJfvtwO5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "agent = '⚽'\n",
        "opponent = '👕'\n",
        "goal = '🥅'\n",
        "\n",
        "arena = [['⚽', ' ' , '👕', ' ' ],\n",
        "         [' ' , ' ' , ' ' , '👕'],\n",
        "         [' ' , '👕', ' ' , ' ' ],\n",
        "         [' ' , ' ' , ' ' , '👕'],\n",
        "         [' ' , '👕', ' ' , '🥅']]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlETEt7LtwO5"
      },
      "source": [
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, f\"Map {map} does not specify an agent {agent} location\"\n",
        "        assert self.goal_state is not None,  f\"Map {map} does not specify a goal {goal} location\"\n",
        "        assert self.opponents_states,  f\"Map {map} does not specify any opponents {opponent} location\"\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the location of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags terminal states.\n",
        "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "        self.done = False\n",
        "        self.actions = ['n','e','w','s']\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, f'Key {key} missing from reward.'\n",
        "        self.rewards = rewards\n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "        # There's really just two things we need to reset: the state, which should\n",
        "        # be reset to the initial state, and the `done` flag which should be \n",
        "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "        # were earlier. \n",
        "        self.state = self.init_state\n",
        "        self.done  = False\n",
        "        return self.state\n",
        "    \n",
        "    def __get_next_state_on_action__(self,state,action):\n",
        "        \"\"\"Return next state based on current state and action.\"\"\"\n",
        "        assert not self.__is_terminal_state__(state), f\"Action {action} undefined for terminal state {state}\"\n",
        "        \n",
        "        row, col = self.__to_indices__(state)\n",
        "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "        row_delta, col_delta = action_to_index_delta[action]\n",
        "        new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "        ## Return current state if next state is invalid\n",
        "        if not(0<=new_row<self.n_rows) or\\\n",
        "        not(0<=new_col<self.n_cols):\n",
        "            return state  \n",
        "\n",
        "        ## Construct state from new row and col and return it.    \n",
        "        return self.__to_state__(new_row, new_col)    \n",
        "    \n",
        "  \n",
        "    def __get_reward_for_transition__(self,state,next_state):\n",
        "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
        "        ## Transition rejected due to illegal action (move)\n",
        "        assert not self.__is_terminal_state__(state), f\"Reward is undefined for terminal state {state}\"\n",
        "        \n",
        "        if next_state == state:\n",
        "            reward = self.rewards['outside']\n",
        "\n",
        "        ## Goal!\n",
        "        elif next_state == self.goal_state:\n",
        "            reward = self.rewards['goal']\n",
        "\n",
        "        ## Ran into opponent. \n",
        "        elif next_state in self.opponents_states:\n",
        "            reward = self.rewards['opponent']\n",
        "\n",
        "        ## Made a safe and valid move.   \n",
        "        else:\n",
        "            reward = self.rewards['unmarked']\n",
        "\n",
        "        return reward    \n",
        "    \n",
        "    \n",
        "    def __is_terminal_state__(self, state):\n",
        "        return (state == self.goal_state) or (state in self.opponents_states) \n",
        "    \n",
        "      \n",
        "    def step(self,action):\n",
        "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
        "        assert not self.done, \\\n",
        "        f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.'\n",
        "        next_state = self.__get_next_state_on_action__(self.state, action)\n",
        "\n",
        "        reward = self.__get_reward_for_transition__(self.state, next_state)\n",
        "\n",
        "        done = self.__is_terminal_state__(next_state)\n",
        "\n",
        "        self.state, self.done = next_state, done\n",
        "\n",
        "        return next_state, reward, done\n",
        "    \n",
        "    \n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Pretty-print the environment and agent.\"\"\"\n",
        "        ## Create a copy of the map and change data type to accomodate\n",
        "        ## 3-character strings\n",
        "        _map = np.array(self.map, dtype='<U3')\n",
        "\n",
        "        ## Mark unoccupied positions with special symbol.\n",
        "        ## And add extra spacing to align all columns.\n",
        "        for row in range(_map.shape[0]):\n",
        "            for col in range(_map.shape[1]):\n",
        "                if _map[row,col] == ' ':\n",
        "                    _map[row,col] = ' + '\n",
        "\n",
        "                elif _map[row,col] == self.opponent_repr: \n",
        "                    _map[row,col] =  self.opponent_repr + ' '\n",
        "\n",
        "                elif _map[row,col] == self.goal_repr:\n",
        "                    _map[row,col] = ' ' + self.goal_repr + ' '\n",
        "\n",
        "        ## If current state overlaps with the goal state or one of the opponents'\n",
        "        ## states, susbstitute a distinct marker.\n",
        "        if self.state == self.goal_state:\n",
        "            r,c = self.__to_indices__(self.state)\n",
        "            _map[r,c] = ' 🏁 '\n",
        "        elif self.state in self.opponents_states:\n",
        "            r,c = self.__to_indices__(self.state)\n",
        "            _map[r,c] = ' ❗ '\n",
        "        else:\n",
        "            r,c = self.__to_indices__(self.state)\n",
        "            _map[r,c] = ' ' + self.agent_repr\n",
        "\n",
        "        for row in range(_map.shape[0]):\n",
        "            for col in range(_map.shape[1]):\n",
        "                print(f' {_map[row,col]} ',end=\"\")\n",
        "            print('\\n') \n",
        "\n",
        "        print()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXzLFTntwO7"
      },
      "source": [
        "foolsball = Foolsball(arena, agent, opponent, goal)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nn8RNR1NDZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f7dcc4-3f6d-4f3e-c341-4f5b57261f1e"
      },
      "source": [
        "foolsball.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  ⚽   +   👕    +  \n",
            "\n",
            "  +    +    +   👕  \n",
            "\n",
            "  +   👕    +    +  \n",
            "\n",
            "  +    +    +   👕  \n",
            "\n",
            "  +   👕    +    🥅  \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9UlwAGrtwO9"
      },
      "source": [
        "# Override the default reward structure.\n",
        "- Use a more sparse reward: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqOIteORtwO9"
      },
      "source": [
        "## Update reward structure to: {'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "foolsball.reset()\n",
        "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ohHJG6twO9"
      },
      "source": [
        "# Implement discounted returns¶\n",
        "$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$where $R_{t_k}$ is the reward after step k and $\\gamma$ is called the discount factor.\n",
        "- Set the discount factor $\\gamma$ to 0.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZx2cfFtwO9"
      },
      "source": [
        "def get_discounted_return(path, gamma=0):\n",
        "    foolsball.reset()\n",
        "    foolsball.render()\n",
        "    _return_ = 0\n",
        "    discount_coeff = 1\n",
        "    for act in path: \n",
        "        next_state, reward, done = foolsball.step(act)\n",
        "        _return_ += discount_coeff*reward\n",
        "        discount_coeff *= gamma    \n",
        "\n",
        "        foolsball.render()\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    print(f'Return (accumulated reward): {_return_}')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qBbq29ftwO-"
      },
      "source": [
        "HYPER_PARAMS = {'gamma':0.9}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utSSpzPYtwO-"
      },
      "source": [
        "# Use dynamic programming to fill up the returns table\n",
        "- The **highest discounted return** for a **(state, action)** can be defined in terms of returns of the next state.\n",
        "\n",
        "$ Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\gamma * \\max \\begin{bmatrix} Return(state_{t+1}, action_{t+1}=='n')\\\\ Return(state_{t+1}, action_{t+1}=='e')\\\\  Return(state_{t+1}, action_{t+1}=='w')\\\\  Return(state_{t+1}, action_{t+1}=='s') \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCV0jwA1twO-"
      },
      "source": [
        "import pandas as pd\n",
        "def make_returns_table(states_list, actions_list, terminal_states):\n",
        "    \"\"\"Create an empty returns table where each entry is initialized arbitrarily.\"\"\"\n",
        "    table = pd.DataFrame.from_dict({s:{a:0 for a in actions_list} for s in states_list}, orient='index')\n",
        "    return table"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZB6nNtitwO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "ca6b724a-cbc9-4fda-d41e-d770f383580f"
      },
      "source": [
        "terminal_states = foolsball.opponents_states + [foolsball.goal_state]\n",
        "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
        "RETURNS_TBL"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>e</th>\n",
              "      <th>w</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    n  e  w  s\n",
              "0   0  0  0  0\n",
              "1   0  0  0  0\n",
              "2   0  0  0  0\n",
              "3   0  0  0  0\n",
              "4   0  0  0  0\n",
              "5   0  0  0  0\n",
              "6   0  0  0  0\n",
              "7   0  0  0  0\n",
              "8   0  0  0  0\n",
              "9   0  0  0  0\n",
              "10  0  0  0  0\n",
              "11  0  0  0  0\n",
              "12  0  0  0  0\n",
              "13  0  0  0  0\n",
              "14  0  0  0  0\n",
              "15  0  0  0  0\n",
              "16  0  0  0  0\n",
              "17  0  0  0  0\n",
              "18  0  0  0  0\n",
              "19  0  0  0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxL3MLDbtwO_"
      },
      "source": [
        "def compute_returns(table,state,action, debug=False): \n",
        "    \"\"\" Recursively compute the discounted return for a (state,action) pair\"\"\"\n",
        "    if not foolsball.__is_terminal_state__(state):\n",
        "\n",
        "        next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "        reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "\n",
        "        update = HYPER_PARAMS['gamma'] *\\\n",
        "        max(table.loc[next_state, foolsball.actions[0]],\\\n",
        "        table.loc[next_state, foolsball.actions[1]],\\\n",
        "        table.loc[next_state, foolsball.actions[2]],\\\n",
        "        table.loc[next_state, foolsball.actions[3]])\n",
        "\n",
        "        table.loc[state, action]  = reward + update\n",
        "    \n",
        "    return table.loc[state,action]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncMk1NfotwPA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "d82af997-7d51-4899-df7a-3fa9022b6b4e"
      },
      "source": [
        "for s in range(foolsball.n_states):\n",
        "    for a in foolsball.actions:\n",
        "        compute_returns(RETURNS_TBL,state=s, action=a, debug=True)\n",
        "RETURNS_TBL"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>e</th>\n",
              "      <th>w</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>-5.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      n    e    w    s\n",
              "0  -1.0  0.0 -1.0  0.0\n",
              "1  -1.0 -5.0  0.0  0.0\n",
              "2   0.0  0.0  0.0  0.0\n",
              "3  -1.0 -1.0 -5.0 -5.0\n",
              "4   0.0  0.0 -1.0  0.0\n",
              "5   0.0  0.0  0.0 -5.0\n",
              "6  -5.0 -5.0  0.0  0.0\n",
              "7   0.0  0.0  0.0  0.0\n",
              "8   0.0 -5.0 -1.0  0.0\n",
              "9   0.0  0.0  0.0  0.0\n",
              "10  0.0  0.0 -5.0  0.0\n",
              "11 -5.0 -1.0  0.0 -5.0\n",
              "12  0.0  0.0 -1.0  0.0\n",
              "13 -5.0  0.0  0.0 -5.0\n",
              "14  0.0 -5.0  0.0  0.0\n",
              "15  0.0  0.0  0.0  0.0\n",
              "16  0.0 -5.0 -1.0 -1.0\n",
              "17  0.0  0.0  0.0  0.0\n",
              "18  0.0  5.0 -5.0  3.5\n",
              "19  0.0  0.0  0.0  0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTOb8hibtwPA"
      },
      "source": [
        "# Let the returns stabilize (converge)¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mle1FbRCtwPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b2f3e1-074d-413c-dbae-e9cb005716c6"
      },
      "source": [
        "RETURNS_TBL = make_returns_table(range(foolsball.n_states), foolsball.actions, terminal_states)\n",
        "for i in range(1,50):\n",
        "    RETURNS_TBL_OLD = RETURNS_TBL.copy()\n",
        "    for s in range(foolsball.n_states):\n",
        "        for a in foolsball.actions:\n",
        "            compute_returns(RETURNS_TBL,state=s, action=a, debug=True)\n",
        "    \n",
        "    if i%5 == 0:\n",
        "        print(f'\\n{i} iterations')\n",
        "        print(RETURNS_TBL)\n",
        "    \n",
        "    deltas = RETURNS_TBL- RETURNS_TBL_OLD\n",
        "    if abs(deltas.values.max()) < 1e-3:\n",
        "        print(f'\\nConvergence achieved at {i} iterations')\n",
        "        print(RETURNS_TBL)\n",
        "        break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5 iterations\n",
            "          n       e        w        s\n",
            "0  -1.00000  0.0000 -1.00000  0.00000\n",
            "1  -1.00000 -5.0000  0.00000  0.00000\n",
            "2   0.00000  0.0000  0.00000  0.00000\n",
            "3  -4.09510 -4.0951 -5.00000 -5.00000\n",
            "4   0.00000  0.0000 -1.00000  0.00000\n",
            "5   0.00000  3.2805  0.00000 -5.00000\n",
            "6  -5.00000 -5.0000  2.95245  3.64500\n",
            "7   0.00000  0.0000  0.00000  0.00000\n",
            "8   0.00000 -5.0000 -1.00000  3.28050\n",
            "9   0.00000  0.0000  0.00000  0.00000\n",
            "10  3.28050  3.2805 -5.00000  4.05000\n",
            "11 -5.00000  2.2805  3.64500 -5.00000\n",
            "12  2.95245  3.6450  2.28050  2.95245\n",
            "13 -5.00000  4.0500  3.28050 -5.00000\n",
            "14  3.64500 -5.0000  3.64500  4.50000\n",
            "15  0.00000  0.0000  0.00000  0.00000\n",
            "16  3.28050 -5.0000  1.95245  1.95245\n",
            "17  0.00000  0.0000  0.00000  0.00000\n",
            "18  4.05000  5.0000 -5.00000  3.50000\n",
            "19  0.00000  0.0000  0.00000  0.00000\n",
            "\n",
            "Convergence achieved at 9 iterations\n",
            "           n         e         w         s\n",
            "0   1.391485  2.657205  1.391485  2.657205\n",
            "1   1.657205 -5.000000  2.391485  2.952450\n",
            "2   0.000000  0.000000  0.000000  0.000000\n",
            "3  -5.500000 -5.500000 -5.000000 -5.000000\n",
            "4   2.391485  2.952450  1.657205  2.952450\n",
            "5   2.657205  3.280500  2.657205 -5.000000\n",
            "6  -5.000000 -5.000000  2.952450  3.645000\n",
            "7   0.000000  0.000000  0.000000  0.000000\n",
            "8   2.657205 -5.000000  1.952450  3.280500\n",
            "9   0.000000  0.000000  0.000000  0.000000\n",
            "10  3.280500  3.280500 -5.000000  4.050000\n",
            "11 -5.000000  2.280500  3.645000 -5.000000\n",
            "12  2.952450  3.645000  2.280500  2.952450\n",
            "13 -5.000000  4.050000  3.280500 -5.000000\n",
            "14  3.645000 -5.000000  3.645000  4.500000\n",
            "15  0.000000  0.000000  0.000000  0.000000\n",
            "16  3.280500 -5.000000  1.952450  1.952450\n",
            "17  0.000000  0.000000  0.000000  0.000000\n",
            "18  4.050000  5.000000 -5.000000  3.500000\n",
            "19  0.000000  0.000000  0.000000  0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOycLCFtwPA"
      },
      "source": [
        "# Intro to Policies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N7tmzxutwPB"
      },
      "source": [
        "def greedy_policy_from_returns_tbl(table):\n",
        "    policy = {s:None for s in table.index }\n",
        "    for state in table.index:\n",
        "        if state not in terminal_states:\n",
        "            greedy_action = table.loc[state].idxmax()\n",
        "            policy[state] = greedy_action\n",
        "            \n",
        "    return policy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pof9FZJRtwPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee1af15-e1c2-486f-e9ac-d29127fa45ca"
      },
      "source": [
        "policy0 = greedy_policy_from_returns_tbl(RETURNS_TBL)\n",
        "policy0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'e',\n",
              " 1: 's',\n",
              " 2: None,\n",
              " 3: 'w',\n",
              " 4: 'e',\n",
              " 5: 'e',\n",
              " 6: 's',\n",
              " 7: None,\n",
              " 8: 's',\n",
              " 9: None,\n",
              " 10: 's',\n",
              " 11: 'w',\n",
              " 12: 'e',\n",
              " 13: 'e',\n",
              " 14: 's',\n",
              " 15: None,\n",
              " 16: 'n',\n",
              " 17: None,\n",
              " 18: 'e',\n",
              " 19: None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPHKvSO-twPB"
      },
      "source": [
        "def pretty_print_policy(policy):\n",
        "    direction_repr = {'n':' 🡑 ', 'e':' 🡒 ', 'w':' 🡐 ', 's':' 🡓 ', None:' ⬤ '}\n",
        "\n",
        "    for row in range(foolsball.n_rows):\n",
        "        for col in range(foolsball.n_cols):\n",
        "            state = foolsball.__to_state__(row, col)\n",
        "            print(direction_repr[policy[state]],end='')\n",
        "        print()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syIQcOxutwPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb26e95-f096-48a9-f611-f024e26beedb"
      },
      "source": [
        "pretty_print_policy(policy0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 🡒  🡓  ⬤  🡐 \n",
            " 🡒  🡒  🡓  ⬤ \n",
            " 🡓  ⬤  🡓  🡐 \n",
            " 🡒  🡒  🡓  ⬤ \n",
            " 🡑  ⬤  🡒  ⬤ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_1BMiVOtwPB"
      },
      "source": [
        "# Dealing with incomplete Knowledge of the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlAYJa2_twPC"
      },
      "source": [
        "import numpy as np\n",
        "def collect_random_episode():\n",
        "    state = foolsball.reset()\n",
        "    done = False\n",
        "    episode = []\n",
        "    \n",
        "    while not done:\n",
        "        action = np.random.choice(foolsball.actions)\n",
        "        next_state, reward, done = foolsball.step(action)\n",
        "        episode.append([state, action, reward])\n",
        "        state = next_state\n",
        "        \n",
        "    return episode"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raFvMPNbtwPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65abd19c-e57f-4b95-f69c-c02a9a925ec3"
      },
      "source": [
        "ep = collect_random_episode()\n",
        "foolsball.render()\n",
        "print(ep)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  +    +    ❗    +  \n",
            "\n",
            "  +    +    +   👕  \n",
            "\n",
            "  +   👕    +    +  \n",
            "\n",
            "  +    +    +   👕  \n",
            "\n",
            "  +   👕    +    🥅  \n",
            "\n",
            "\n",
            "[[0, 'w', -1], [0, 'e', 0], [1, 'e', -5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lem6V-HKtwPC"
      },
      "source": [
        "# Implement discounted returns for episodes\n",
        "- If an episode is: (s1,a1,r1),(s2,a2,r2),(s3,a3,r3), (s4),  s4 being a terminal state:\n",
        "  - The (discounted) return for (s1,a1) is r1+γ∗r2+γ2∗r3\n",
        "  - The (discounted) return for (s2, a2)is r2+γ∗r3\n",
        "  - The (discounted) return for (s3,a3) is r3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBeJkSUFtwPC"
      },
      "source": [
        "def discounted_return_from_episode(ep, gamma=0):\n",
        "    states, actions, rewards = list(zip(*ep))\n",
        "    rewards = np.asarray(rewards)\n",
        "    discount_coeffs = np.asarray([np.power(gamma,p) for p in range(len(rewards))])\n",
        "    \n",
        "    l = len(rewards)\n",
        "    discounted_returns = [np.dot(rewards[i:],discount_coeffs[:l-i]) for i in range(l)]\n",
        "    \n",
        "    return (states, actions, discounted_returns)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3YOuaUrtwPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bcbf7c-6182-457f-f1b6-cacbe75f6d02"
      },
      "source": [
        "discounted_return_from_episode(ep, gamma=HYPER_PARAMS['gamma'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 0, 1), ('w', 'e', 'e'), [-5.050000000000001, -4.5, -5.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6WGX90itwPC"
      },
      "source": [
        "# Exploration-Exploitation with Epsilon Decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ui6QvsTtwPD"
      },
      "source": [
        "def collect_epsilon_greedy_episode_from_returns_tbl(table, max_ep_len=20, epsilon=0.1):\n",
        "    state = foolsball.reset()\n",
        "    done = False\n",
        "    episode = []\n",
        "    \n",
        "    for _ in range(max_ep_len):\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "        actions = table.columns\n",
        "        action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "\n",
        "        greedy_action_index = np.argmax(table.loc[state].values)\n",
        "        action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "        epsilon_greedy_action = np.random.choice(table.columns,p=action_probs)\n",
        "\n",
        "        next_state, reward, done = foolsball.step(epsilon_greedy_action)\n",
        "        episode.append([state, epsilon_greedy_action, reward])\n",
        "        state = next_state\n",
        "\n",
        "    return episode"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdnCHP9ctwPD"
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 5000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "for i in range(n_episodes):\n",
        "    estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
        "  \n",
        "    epsilon = max(epsilon, min_epsilon)\n",
        "    episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
        "    epsilon *= epsilon_decay\n",
        "    #print(episode_i)\n",
        "    states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "    for s,a,ret in zip(states, actions, discounted_returns):\n",
        "        ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
        "        VISITS_COUNTS_TBL.loc[s,a] += 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdOPuSTVtwPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d576021e-2eb7-43ed-d73f-6367f686adcd"
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "print(estimated_returns)\n",
        "\n",
        "policy4 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "print(policy4)\n",
        "\n",
        "pretty_print_policy(policy4)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           n         e         w         s\n",
            "0  -2.029627 -1.288397 -2.008482 -0.378704\n",
            "1  -2.992107 -4.978632 -1.964128 -0.915024\n",
            "2   0.000000  0.000000  0.000000  0.000000\n",
            "3   0.000000  0.000000  0.000000  0.000000\n",
            "4  -0.364071 -1.086082 -1.836632 -1.051419\n",
            "5  -1.783415 -2.055021 -0.640600 -4.982079\n",
            "6  -4.880952 -4.878049 -1.315188 -2.118011\n",
            "7   0.000000  0.000000  0.000000  0.000000\n",
            "8  -0.723996 -4.970414 -2.711965 -1.015609\n",
            "9   0.000000  0.000000  0.000000  0.000000\n",
            "10 -2.720409 -2.130805 -4.166667 -0.765638\n",
            "11 -3.750000 -3.816667 -1.169471 -3.750000\n",
            "12 -2.110119 -0.175032 -2.910631 -2.656098\n",
            "13 -4.500000  1.007316 -1.767413 -4.375000\n",
            "14 -0.127181 -4.545455 -2.356286  1.904575\n",
            "15  0.000000  0.000000  0.000000  0.000000\n",
            "16 -1.362556 -4.615385 -3.198493 -3.844303\n",
            "17  0.000000  0.000000  0.000000  0.000000\n",
            "18 -0.121500  4.880952 -4.545455  3.111111\n",
            "19  0.000000  0.000000  0.000000  0.000000\n",
            "{0: 's', 1: 's', 2: None, 3: 'n', 4: 'n', 5: 'w', 6: 'w', 7: None, 8: 'n', 9: None, 10: 's', 11: 'w', 12: 'e', 13: 'e', 14: 's', 15: None, 16: 'n', 17: None, 18: 'e', 19: None}\n",
            " 🡓  🡓  ⬤  🡑 \n",
            " 🡑  🡐  🡐  ⬤ \n",
            " 🡑  ⬤  🡓  🡐 \n",
            " 🡒  🡒  🡓  ⬤ \n",
            " 🡑  ⬤  🡒  ⬤ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gksL6PLntwPD"
      },
      "source": [
        "# Constant Alpha\n",
        "\n",
        "## The idea:\n",
        "- Dividing the accumulated returns by visit count has a non linear effect on the updates. (Go back to previous step and see for yourself).\n",
        "- Don't divide at all!\n",
        "- But we need to ensure that updates are small\n",
        "- Idea:\n",
        " - ESTIMATED_RETURNS_TBL.loc[s,a] and ret are both estimates of the same quantity.\n",
        " - Use the difference of the two estimates to update ESTIMATED_RETURNS_TBL.loc[s,a] much like we do in Deep Learning.\n",
        "\n",
        "Todo:\n",
        "- Complete the missing code in the next cell.\n",
        "- Run the next few cells to get a policy and evaluate it.\n",
        "- Does the policy help the agent attain its goal?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQhSLmUNtwPD"
      },
      "source": [
        "# SARSA\n",
        "\n",
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 5000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "alpha = 0.001\n",
        "\n",
        "table = ESTIMATED_RETURNS_TBL\n",
        "\n",
        "i = 1\n",
        "done  = False\n",
        "s_t = foolsball.reset()\n",
        "a_t = np.random.choice(table.columns)\n",
        "\n",
        "while i < n_episodes:\n",
        "  \n",
        "    if done:\n",
        "        #foolsball.reset()\n",
        "        s_t = foolsball.reset()\n",
        "        done = False\n",
        "        epsilon *= epsilon_decay\n",
        "        i += 1\n",
        "        epsilon = max(epsilon,min_epsilon)\n",
        "\n",
        "        actions = table.columns\n",
        "        action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "\n",
        "        greedy_action_index = np.argmax(table.loc[s_t].values)\n",
        "        action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "        a_t = np.random.choice(table.columns,p=action_probs)\n",
        "\n",
        "    epsilon = max(epsilon,min_epsilon)\n",
        "\n",
        "    s_tp1, r_tp1, done = foolsball.step(a_t)\n",
        "\n",
        "    actions = table.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(table.loc[s_tp1].values)\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "    a_tp1 = np.random.choice(table.columns,p=action_probs)\n",
        "\n",
        "\n",
        "    gamma=HYPER_PARAMS['gamma']\n",
        "\n",
        "    # SARSA\n",
        "    ESTIMATED_RETURNS_TBL.loc[s_t, a_t] += alpha*(r_tp1 + gamma*ESTIMATED_RETURNS_TBL.loc[s_tp1,a_tp1]- ESTIMATED_RETURNS_TBL.loc[s_t,a_t])\n",
        "\n",
        "    s_t = s_tp1\n",
        "    a_t = a_tp1\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTlVkh-LF2RQ"
      },
      "source": [
        "# Q-learning\n",
        "\n",
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 5000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "alpha = 0.001\n",
        "\n",
        "table = ESTIMATED_RETURNS_TBL\n",
        "\n",
        "i = 1\n",
        "done  = False\n",
        "s_t = foolsball.reset()\n",
        "a_t = np.random.choice(table.columns)\n",
        "\n",
        "while i < n_episodes:\n",
        "    \n",
        "    if done: # start new episode\n",
        "        #foolsball.reset()\n",
        "        s_t = foolsball.reset()\n",
        "        done = False\n",
        "        if i>500: # do exploration only until 500th episode\n",
        "          epsilon *= epsilon_decay\n",
        "        i += 1\n",
        "        epsilon = max(epsilon,min_epsilon)\n",
        "        #episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
        "\n",
        "        actions = table.columns\n",
        "        action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "\n",
        "        greedy_action_index = np.argmax(table.loc[s_t].values)\n",
        "        action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "        a_t = np.random.choice(table.columns,p=action_probs)\n",
        "\n",
        "    epsilon = max(epsilon,min_epsilon)\n",
        "\n",
        "    actions = table.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "\n",
        "    greedy_action_index = np.argmax(table.loc[s_t].values)\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "    a_t = np.random.choice(table.columns,p=action_probs)\n",
        "\n",
        "    gamma=HYPER_PARAMS['gamma']\n",
        "\n",
        "    s_tp1, r_tp1, done = foolsball.step(a_t)\n",
        "\n",
        "    # Q-learning\n",
        "    max_return = table.loc[s_tp1].max()\n",
        "    ESTIMATED_RETURNS_TBL.loc[s_t, a_t] += alpha*(r_tp1 + gamma*max_return- ESTIMATED_RETURNS_TBL.loc[s_t,a_t])\n",
        "\n",
        "    s_t = s_tp1\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i194SI2itwPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa87dfb9-f551-4a54-de22-20f99f4fc0f2"
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL\n",
        "print(estimated_returns)\n",
        "\n",
        "policy5 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "print(policy5)\n",
        "\n",
        "pretty_print_policy(policy5)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               n             e             w             s\n",
            "0  -6.106818e-01  1.144246e-01 -6.276689e-01  5.843421e-04\n",
            "1  -4.266872e-01 -2.170349e+00  1.649476e-03  2.869030e-01\n",
            "2   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "3   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "4   3.969400e-05  2.878204e-02 -3.364568e-01  7.852333e-10\n",
            "5   5.329032e-03  6.499995e-01  4.182445e-04 -1.594997e+00\n",
            "6  -1.145244e+00 -1.083040e+00  1.070606e-02  1.289193e+00\n",
            "7   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "8   1.753846e-05 -5.922037e-01 -1.530241e-01  3.456395e-14\n",
            "9   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "10  2.352119e-02  1.343163e-03 -8.614588e-01  2.290378e+00\n",
            "11 -9.905568e-02 -1.580721e-02  6.607392e-02 -1.381263e-01\n",
            "12  5.208108e-09  4.610568e-21 -6.950263e-02  5.496647e-20\n",
            "13 -1.235114e-01  1.165058e-01  2.617301e-12 -1.478452e-01\n",
            "14  6.744956e-02 -7.693555e-01  3.054988e-03  3.522150e+00\n",
            "15  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "16  4.992534e-17 -1.720575e-01 -2.956903e-02 -1.686468e-02\n",
            "17  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "18  1.346076e-01  4.698494e+00 -5.434257e-01  1.593101e-01\n",
            "19  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00\n",
            "{0: 'e', 1: 's', 2: None, 3: 'n', 4: 'e', 5: 'e', 6: 's', 7: None, 8: 'n', 9: None, 10: 's', 11: 'w', 12: 'n', 13: 'e', 14: 's', 15: None, 16: 'n', 17: None, 18: 'e', 19: None}\n",
            " 🡒  🡓  ⬤  🡑 \n",
            " 🡒  🡒  🡓  ⬤ \n",
            " 🡑  ⬤  🡓  🡐 \n",
            " 🡑  🡒  🡓  ⬤ \n",
            " 🡑  ⬤  🡒  ⬤ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRH33bi9twPE"
      },
      "source": [
        "# How can we get faster convergence?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY4eFmJUtwPE"
      },
      "source": [
        "- Try the SARSA and Q-learning appraches described [here](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control) "
      ]
    }
  ]
}